{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f88902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import cos,sin,pi\n",
    "from numpy.random import random,randint,choice,sample\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import glob,cv2,time\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers,Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46b0f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense image warp from Tensorflow addons\n",
    "# There is an error using tfa.image.dense_image_warp\n",
    "# define same tfa.image.dense_image_warp\n",
    "\n",
    "from tensorflow_addons.utils import types\n",
    "from typing import Optional\n",
    "\n",
    "def _get_dim(x, idx):\n",
    "    if x.shape.ndims is None:\n",
    "        return tf.shape(x)[idx]\n",
    "    return x.shape[idx] or tf.shape(x)[idx]\n",
    "\n",
    "def dense_image_warp(image: types.TensorLike, displacement: types.TensorLike, name: Optional[str] = None) -> tf.Tensor:\n",
    "    with tf.name_scope(name or \"dense_image_warp\"):\n",
    "        image = tf.convert_to_tensor(image)\n",
    "        displacement = tf.convert_to_tensor(displacement)\n",
    "        batch_size, height, width, channels = (\n",
    "            _get_dim(image, 0),\n",
    "            _get_dim(image, 1),\n",
    "            _get_dim(image, 2),\n",
    "            _get_dim(image, 3),\n",
    "        )\n",
    "\n",
    "        # The flow is defined on the image grid. Turn the flow into a list of query\n",
    "        # points in the grid space.\n",
    "        grid_x, grid_y = tf.meshgrid(tf.range(width), tf.range(height))\n",
    "        stacked_grid = tf.cast(tf.stack([grid_y, grid_x], axis=2), displacement.dtype)\n",
    "        batched_grid = tf.expand_dims(stacked_grid, axis=0)\n",
    "        query_points_on_grid = batched_grid + displacement\n",
    "        query_points_flattened = tf.reshape(query_points_on_grid, [batch_size, height * width, 2])\n",
    "        # Compute values at the query points, then reshape the result back to the\n",
    "        # image grid.\n",
    "        interpolated = tfa.image.interpolate_bilinear(image, query_points_flattened)\n",
    "        interpolated = tf.reshape(interpolated, [batch_size, height, width, channels])\n",
    "        return interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c876fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_grad(imgs):\n",
    "    #central different\n",
    "    img_x,img_y = tf.image.image_gradients(imgs)\n",
    "    img_x,img_y = (img_x+tf.roll(img_x,1,1))/2, (img_y+tf.roll(img_y,1,2))/2\n",
    "    return img_x,img_y\n",
    "def count_fold_fn(u):\n",
    "    u = tf.convert_to_tensor(u)\n",
    "    u_x,u_y = img_grad(u)\n",
    "    det_u = (u_x[:,:,:,0]+1)*(u_y[:,:,:,1]+1)-u_x[:,:,:,1]*u_y[:,:,:,0]\n",
    "    cf = tf.math.count_nonzero(det_u<=0,axis=[1,2])\n",
    "    return tf.reduce_mean(cf).numpy()\n",
    "def grid_warp(u,n=48):\n",
    "    img_shape = u.shape[:-1]\n",
    "    img = np.zeros(img_shape)\n",
    "    for i in range(1,n):\n",
    "        img[img_shape[0]*i//n,:]=1\n",
    "        img[:,img_shape[0]*i//n]=1\n",
    "    ex_img = np.expand_dims(np.expand_dims(img,-1),0)\n",
    "    ex_u = np.expand_dims(u,0)\n",
    "    warp = tf.squeeze(dense_image_warp(ex_img,ex_u))\n",
    "    plt.imshow(warp,cmap='gray')\n",
    "def vector_field(u,n=6):\n",
    "    J,I = np.meshgrid(np.arange(256),np.arange(256))\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.quiver(J[::n,::n],I[::n,::n],u[::n,::n,1],-u[::n,::n,0],units='xy')\n",
    "def show_result(moved_img,fixed_img,Ex_name):\n",
    "    stack = np.expand_dims(np.stack([moved_img,fixed_img],-1),0)\n",
    "    start = time.time()\n",
    "    u = model(stack)\n",
    "    print(time.time()-start)\n",
    "    warped_img = tf.squeeze(dense_image_warp(np.expand_dims(np.expand_dims(moved_img,-1),0),u))\n",
    "    nof = count_fold_fn(u)\n",
    "    nof_inv = count_fold_fn(-u)\n",
    "    rel_ssd = np.sum((warped_img-fixed_img)**2)/np.sum((moved_img-fixed_img)**2)\n",
    "    dcs = D_DCS(warped_img,fixed_img,axis=[0,1]).numpy()\n",
    "    print('folding  u=',nof)\n",
    "    print('folding -u=',nof_inv)\n",
    "    print('relative ssd =',rel_ssd)\n",
    "    print('Dice score =',dcs)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.subplot(241)\n",
    "    plt.title(Ex_name+': Moved image')\n",
    "    plt.imshow(moved_img,cmap='gray')\n",
    "    plt.subplot(242)\n",
    "    plt.title(Ex_name+': Fixed image')\n",
    "    plt.imshow(fixed_img,cmap='gray')\n",
    "    plt.subplot(243)\n",
    "    plt.title(Ex_name+': Warped image')\n",
    "    plt.imshow(warped_img,cmap='gray')\n",
    "    plt.subplot(244)\n",
    "    plt.title(Ex_name+': Different image')\n",
    "    plt.imshow(abs(warped_img-fixed_img),cmap='gray')\n",
    "    plt.subplot(245)\n",
    "    plt.title(Ex_name+': Grid warped u')\n",
    "    grid_warp(u[0])\n",
    "    plt.subplot(246)\n",
    "    plt.title(Ex_name+': Vector field u')\n",
    "    vector_field(-u[0])\n",
    "    plt.subplot(247)\n",
    "    plt.title(Ex_name+': Grid warped -u')\n",
    "    grid_warp(-u[0])\n",
    "    plt.subplot(248)\n",
    "    plt.title(Ex_name+': Vector field -u')\n",
    "    vector_field(u[0])\n",
    "    plt.show()\n",
    "    return rel_ssd,nof,dcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a95012",
   "metadata": {},
   "source": [
    "# Train/Test spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07b53bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "data_set = np.load('550m2f_Diffeomorphic_Deform.npy')\n",
    "train_set = data_set[:500,:,:,:]\n",
    "test_set = data_set[500:550,:,:,:]\n",
    "train_size = train_set.shape[0]\n",
    "test_size = test_set.shape[0]\n",
    "img_shape = train_set.shape[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1846d7d3",
   "metadata": {},
   "source": [
    "# Loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96e6e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_MSE(img1,img2):\n",
    "    mse = tf.reduce_mean(tf.square(img1-img2),axis=[1,2,3])\n",
    "    return tf.cast(tf.reduce_mean(mse),tf.float32)\n",
    "\n",
    "def D_SSD(img1,img2):\n",
    "    ssd = tf.reduce_sum(tf.square(img1-img2),axis=[1,2,3])\n",
    "    return tf.cast(tf.reduce_mean(ssd),tf.float32)\n",
    "\n",
    "def D_CC(img1,img2,n=9):\n",
    "    gs_img1 = tfa.image.gaussian_filter2d(img1,filter_shape=(n,n))\n",
    "    gs_img2 = tfa.image.gaussian_filter2d(img2,filter_shape=(n,n))\n",
    "    dif_img1 = img1-gs_img1\n",
    "    dif_img2 = img2-gs_img2\n",
    "    cc = tf.reduce_sum(dif_img1*dif_img2,axis=[1,2,3])**2\n",
    "    cc = cc/(tf.reduce_sum(dif_img1**2,axis=[1,2,3])*tf.reduce_sum(dif_img2**2,axis=[1,2,3]))\n",
    "    return tf.cast(tf.reduce_mean(cc),tf.float32)\n",
    "\n",
    "def D_NGF(img1,img2,e=1):\n",
    "    img1_x,img1_y = img_grad(img1)\n",
    "    img2_x,img2_y = img_grad(img2)\n",
    "    inner = lambda a,b: a*b+e**2\n",
    "    norm_s = lambda a,b: inner(a,a)+inner(b,b)\n",
    "    ngf = 1 - ((inner(img1_x,img2_x)+inner(img1_y,img2_y))**2)/(norm_s(img1_x,img1_y)*norm_s(img2_x,img2_y))\n",
    "    return tf.reduce_mean(tf.reduce_sum(ngf,axis=[1,2,3]))\n",
    "\n",
    "def D_CLM(img1,img2):\n",
    "    img1 = tf.convert_to_tensor(img1)\n",
    "    img2 = tf.convert_to_tensor(img2)\n",
    "    b,h,w,c = img1.get_shape()\n",
    "    tile_fn = lambda b_c: tf.transpose(tf.reshape(tf.tile(b_c,(h,w)),[h,b,w,c]),[1,0,2,3])\n",
    "    img1_mean = tile_fn(tf.math.reduce_mean(img1,axis=[1,2]))\n",
    "    img2_mean = tile_fn(tf.math.reduce_mean(img2,axis=[1,2]))\n",
    "    img1_std = tile_fn(tf.math.reduce_std(img1,axis=[1,2]))\n",
    "    img2_std = tile_fn(tf.math.reduce_std(img2,axis=[1,2]))\n",
    "    img12_std = tile_fn(tf.math.reduce_std(img1+img2,axis=[1,2]))\n",
    "    clm = tf.reduce_sum(((img1-img1_mean)/img1_std-(img2-img2_mean)/img2_std)**2,axis=(1,2,3))+(img1_std+img2_std-img12_std)**2\n",
    "    return tf.cast(tf.reduce_mean(clm),tf.float32)\n",
    "\n",
    "def D_DCS(img1,img2,axis=[1,2,3]):\n",
    "    dcs = 2*tf.reduce_sum(img1*img2,axis=axis)/(tf.reduce_sum(img1,axis=axis)+tf.reduce_sum(img2,axis=axis))\n",
    "    return tf.reduce_mean(dcs)\n",
    "    \n",
    "def R_difsn(u):\n",
    "    u_x,u_y = img_grad(u)\n",
    "    difsn = tf.reduce_sum(u_x**2+u_y**2,axis=[1,2,3])\n",
    "    return tf.reduce_mean(difsn)\n",
    "\n",
    "def R_tv(u,e=5e-3):\n",
    "    u_x,u_y = img_grad(u)\n",
    "    tv = tf.reduce_sum((u_x**2+u_y**2+e)**0.5,axis=[1,2,3])\n",
    "    return tf.reduce_mean(tv)\n",
    "\n",
    "def R_curv(u):\n",
    "    u_x,u_y = tf.image.image_gradients(u) #forward different\n",
    "    u_xx = u_x-tf.roll(u_x,1,1)\n",
    "    u_yy = u_y-tf.roll(u_y,1,2)\n",
    "    curv = tf.reduce_sum(u_xx**2+u_yy**2,axis=[1,2,3])\n",
    "    return tf.reduce_mean(curv)\n",
    "\n",
    "def R_HH(u):\n",
    "    u_x,u_y = img_grad(u)\n",
    "    r = (u_x[:,:,:,0]-u_y[:,:,:,1])**2+(u_x[:,:,:,1]+u_y[:,:,:,0])**2\n",
    "    R1 = tf.reduce_sum(r,axis=[1,2])\n",
    "    return tf.reduce_mean(R1)\n",
    "\n",
    "def R_SA(u):\n",
    "    u = tf.convert_to_tensor(u)\n",
    "    u_x,u_y = img_grad(u)\n",
    "    u1_x1 = u_x[:,:,:,0]\n",
    "    u1_x2 = u_x[:,:,:,1]\n",
    "    u2_x1 = u_y[:,:,:,0]\n",
    "    u2_x2 = u_y[:,:,:,1]\n",
    "    s = u1_x2*u2_x1-u1_x1*u2_x2\n",
    "    a = abs(u1_x1+u2_x2)\n",
    "    Rsa = tf.reduce_sum(tf.minimum(0,s+a)**2,axis=[1,2])    \n",
    "    return tf.reduce_mean(Rsa)\n",
    "\n",
    "def R_fold(u):\n",
    "    u = tf.convert_to_tensor(u)\n",
    "    u_x,u_y = img_grad(u)\n",
    "    det_phi = (u_x[:,:,:,0]+1)*(u_y[:,:,:,1]+1)-u_x[:,:,:,1]*u_y[:,:,:,0]\n",
    "    det_phi_p = tf.maximum(det_phi,1)\n",
    "    fold = tf.reduce_sum((det_phi-1)**2/det_phi_p**2,axis=(1,2))\n",
    "    return tf.reduce_mean(fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5ae5050",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Unet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 2) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 32) 608         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    multiple             0           conv2d[0][0]                     \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 64) 18496       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 128)  73856       max_pooling2d[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 128)  147584      max_pooling2d[2][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    multiple             0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       multiple             0           up_sampling2d[0][0]              \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "                                                                 up_sampling2d[1][0]              \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "                                                                 up_sampling2d[2][0]              \n",
      "                                                                 conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 64)   147520      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 128, 128, 32) 36896       concatenate[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 256, 256, 64) 36928       concatenate[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 256, 256, 2)  130         conv2d_6[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 462,018\n",
      "Trainable params: 462,018\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "inputs = layers.Input(shape=train_set[0].shape)\n",
    "Conv2D = lambda l: layers.Conv2D(l,3,activation='relu',padding='same')\n",
    "Maxpool2D = layers.MaxPool2D()\n",
    "UpSampling2D = layers.UpSampling2D()\n",
    "Concatenate = layers.Concatenate()\n",
    "n = 3\n",
    "lys = 32\n",
    "\n",
    "x = inputs\n",
    "skip = []\n",
    "for i in range(n): \n",
    "    x = Conv2D(lys*2**i)(x)\n",
    "    skip.append(x)\n",
    "    x = Maxpool2D(x)\n",
    "for j in range(n):\n",
    "    x = Conv2D(lys*2**(n-j-1))(x)\n",
    "    x = UpSampling2D(x)\n",
    "    x = Concatenate([x,skip.pop()])\n",
    "    \n",
    "x = Conv2D(2*lys)(x)\n",
    "x = tf.keras.layers.Conv2D(2,1,padding='same')(x)\n",
    "model = Model(inputs=inputs, outputs=x,name='Unet')\n",
    "model.summary()\n",
    "weight_save = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef6321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'm1curv3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7882cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model,imgs):\n",
    "    u = model(imgs)\n",
    "    ref_img = imgs[:,:,:,1:]\n",
    "    warped_img = dense_image_warp(imgs[:,:,:,:1],u)\n",
    "    D = D_SSD(ref_img,warped_img)\n",
    "    R = 3*R_curv(u)#+0*R_HH(u)+0*R_SA(u)\n",
    "    #print('No.folding :',count_fold_fn(u))\n",
    "    loss = D+R\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49027d8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.854464\n",
      "memory current : 2.217728, peak : 2.807552\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "epoch 1, loss_train 999.18518, loss_test 1004.42139\n",
      "memory current : 13.013504, peak : 1819.036672\n",
      "epoch 2, loss_train 875.30127, loss_test 832.25\n",
      "memory current : 13.014016, peak : 1819.036672\n",
      "epoch 3, loss_train 599.47064, loss_test 553.52557\n",
      "epoch 4, loss_train 407.35437, loss_test 374.28613\n",
      "epoch 5, loss_train 318.9646, loss_test 314.6228\n",
      "epoch 6, loss_train 281.32462, loss_test 295.92435\n",
      "epoch 7, loss_train 262.57382, loss_test 279.03217\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.get_memory_info('GPU:0')['current']/1e6)\n",
    "model.set_weights(weight_save)\n",
    "epochs = 100\n",
    "batch_size = 10\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "loss_log = []\n",
    "metric_log = []\n",
    "start_train = time.time()\n",
    "for epoch in range(epochs):\n",
    "    if epoch<3:\n",
    "        mem = tf.config.experimental.get_memory_info('GPU:0')\n",
    "        cr,pk = mem['current'],mem['peak']\n",
    "        print('memory current : {}, peak : {}'.format(cr/1e6,pk/1e6)) \n",
    "    loss_epoch = []\n",
    "    test_epoch = []\n",
    "    #train\n",
    "    for i in range(0,train_size,batch_size):\n",
    "        train_batch = train_set[i:i+batch_size]\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_batch = loss_fn(model,train_batch)\n",
    "            grads = tape.gradient(loss_batch, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        loss_epoch.append(loss_batch.numpy())\n",
    "    #test\n",
    "    for j in range(0,test_size,batch_size):\n",
    "        loss_test = loss_fn(model,test_set[j:j+batch_size])\n",
    "        test_epoch.append(loss_batch.numpy())\n",
    "    loss_log = np.append(loss_log,[np.mean(loss_epoch),np.mean(test_epoch)])\n",
    "    \n",
    "    dgt = 5 #display 'dgt' digits\n",
    "    print('epoch {}, loss_train {}, loss_test {}'.format(epoch+1,np.round(loss_log[-2],dgt),np.round(loss_log[-1],dgt)))\n",
    "print(time.time()-start_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f89ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_log = loss_log.reshape(loss_log.size//2,2)\n",
    "plt.plot(loss_log[:,0],label='train')\n",
    "plt.plot(loss_log[:,1],label='test')\n",
    "plt.title(name)\n",
    "plt.ylim(0,max(loss_log[0,:]))\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399467e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ind_list = choice(test_set.shape[0],3,False)\n",
    "for ind in ind_list:\n",
    "    test = np.expand_dims(test_set[ind],0)\n",
    "    moved_img = test[0,:,:,0]\n",
    "    fixed_img = test[0,:,:,1]\n",
    "    show_result(moved_img,fixed_img,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab782647",
   "metadata": {},
   "outputs": [],
   "source": [
    "circle = cv2.resize(cv2.imread('./Images/circle.png',0),img_shape)/255\n",
    "ellipse = cv2.resize(cv2.imread('./Images/ellipse.png',0),img_shape)/255\n",
    "square = cv2.resize(cv2.imread('./Images/square.png',0),img_shape)/255\n",
    "c_shape = cv2.resize(cv2.imread('./Images/c_shape.png',0),img_shape)/255\n",
    "o_ = cv2.resize(cv2.imread('./Images/o_.png',0),img_shape)/255\n",
    "R1 = cv2.resize(cv2.imread('./Images/R1.png',0),img_shape)/255\n",
    "T1 = cv2.resize(cv2.imread('./Images/T1.png',0),img_shape)/255\n",
    "t2_R = cv2.resize(cv2.imread('./Images/t2_R.png',0),img_shape)/255\n",
    "t2_T = cv2.resize(cv2.imread('./Images/t2_T.png',0),img_shape)/255\n",
    "R2 = cv2.resize(cv2.imread('./Images/R2.png',0),img_shape)/255\n",
    "T2 = cv2.resize(cv2.imread('./Images/T2.png',0),img_shape)/255\n",
    "A_slope = cv2.resize(cv2.imread('./Images/A_slope.png',0),img_shape)/255\n",
    "R = cv2.resize(cv2.imread('./Images/R.png',0),img_shape)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2773812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_list = [\n",
    "    [ellipse,circle],\n",
    "    [circle,ellipse],\n",
    "    [circle,square],\n",
    "    [square,circle],\n",
    "    [t2_R,t2_T],\n",
    "    [t2_T,t2_R],\n",
    "    [o_,c_shape],\n",
    "    [c_shape,o_],\n",
    "    [A_slope,R],\n",
    "    [R,A_slope],\n",
    "    [R1,T1],\n",
    "    [T1,R1],\n",
    "    [R2,T2],\n",
    "    [T2,R2]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea59398a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for i in display_list:\n",
    "    result_list.append(show_result(i[0],i[1],name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ac2180",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "result = np.array(result_list).transpose()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca94f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_fold_dir = 0\n",
    "sum_fold_inv = 0\n",
    "rel_ssd_list = []\n",
    "dcs_list = []\n",
    "for j in range(0,test_size,batch_size):\n",
    "    test_batch = test_set[j:j+batch_size,:,:,:]\n",
    "    u = model(test_batch)\n",
    "    moved_imgs = test_batch[:,:,:,:1]\n",
    "    warped_imgs = dense_image_warp(moved_imgs,u)\n",
    "    fixed_imgs = test_batch[:,:,:,1:]\n",
    "    rel_ssd_list.append(tf.reduce_sum((warped_imgs-fixed_imgs)**2,axis=[1,2])/tf.reduce_sum((moved_imgs-fixed_imgs)**2,axis=[1,2]))\n",
    "    \n",
    "    dcs_list.append(D_DCS(warped_imgs,fixed_imgs,axis=[1,2]))\n",
    "    sum_fold_dir+=count_fold_fn(u)\n",
    "    sum_fold_inv+=count_fold_fn(-u)\n",
    "print('rel_ssd :',np.mean(rel_ssd_list))\n",
    "print('No.folding  u per img:',sum_fold_dir/test_size)\n",
    "print('No.folding -u per img:',sum_fold_inv/test_size)\n",
    "print('DCS :',np.mean(dcs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f490a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./loss log/'+name+'.npy',loss_log)\n",
    "model.save_weights('./weight models/'+name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow2-GPU",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

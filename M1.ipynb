{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f88902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import cos,sin,pi\n",
    "from numpy.random import random,randint,choice,sample\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import glob,cv2,time\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers,Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46b0f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense image warp from Tensorflow addons\n",
    "# There is an error using tfa.image.dense_image_warp\n",
    "# define same tfa.image.dense_image_warp\n",
    "\n",
    "from tensorflow_addons.utils import types\n",
    "from typing import Optional\n",
    "\n",
    "def _get_dim(x, idx):\n",
    "    if x.shape.ndims is None:\n",
    "        return tf.shape(x)[idx]\n",
    "    return x.shape[idx] or tf.shape(x)[idx]\n",
    "\n",
    "def dense_image_warp(image: types.TensorLike, displacement: types.TensorLike, name: Optional[str] = None) -> tf.Tensor:\n",
    "    with tf.name_scope(name or \"dense_image_warp\"):\n",
    "        image = tf.convert_to_tensor(image)\n",
    "        displacement = tf.convert_to_tensor(displacement)\n",
    "        batch_size, height, width, channels = (\n",
    "            _get_dim(image, 0),\n",
    "            _get_dim(image, 1),\n",
    "            _get_dim(image, 2),\n",
    "            _get_dim(image, 3),\n",
    "        )\n",
    "\n",
    "        # The flow is defined on the image grid. Turn the flow into a list of query\n",
    "        # points in the grid space.\n",
    "        grid_x, grid_y = tf.meshgrid(tf.range(width), tf.range(height))\n",
    "        stacked_grid = tf.cast(tf.stack([grid_y, grid_x], axis=2), displacement.dtype)\n",
    "        batched_grid = tf.expand_dims(stacked_grid, axis=0)\n",
    "        query_points_on_grid = batched_grid + displacement\n",
    "        query_points_flattened = tf.reshape(query_points_on_grid, [batch_size, height * width, 2])\n",
    "        # Compute values at the query points, then reshape the result back to the\n",
    "        # image grid.\n",
    "        interpolated = tfa.image.interpolate_bilinear(image, query_points_flattened)\n",
    "        interpolated = tf.reshape(interpolated, [batch_size, height, width, channels])\n",
    "        return interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c876fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_grad(imgs):\n",
    "    #central different\n",
    "    img_x,img_y = tf.image.image_gradients(imgs)\n",
    "    img_x,img_y = (img_x+tf.roll(img_x,1,1))/2, (img_y+tf.roll(img_y,1,2))/2\n",
    "    return img_x,img_y\n",
    "def count_fold_fn(u):\n",
    "    u = tf.convert_to_tensor(u)\n",
    "    u_x,u_y = img_grad(u)\n",
    "    det_u = (u_x[:,:,:,0]+1)*(u_y[:,:,:,1]+1)-u_x[:,:,:,1]*u_y[:,:,:,0]\n",
    "    cf = tf.math.count_nonzero(det_u<=0,axis=[1,2])\n",
    "    return cf\n",
    "def grid_warp(u,n=48):\n",
    "    img_shape = u.shape[:-1]\n",
    "    img = np.zeros(img_shape)\n",
    "    for i in range(1,n):\n",
    "        img[img_shape[0]*i//n,:]=1\n",
    "        img[:,img_shape[0]*i//n]=1\n",
    "    ex_img = np.expand_dims(np.expand_dims(img,-1),0)\n",
    "    ex_u = np.expand_dims(u,0)\n",
    "    warp = tf.squeeze(dense_image_warp(ex_img,ex_u))\n",
    "    plt.imshow(warp,cmap='gray')\n",
    "def vector_field(u,n=6):\n",
    "    J,I = np.meshgrid(np.arange(256),np.arange(256))\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.quiver(J[::n,::n],I[::n,::n],u[::n,::n,1],-u[::n,::n,0],units='xy')\n",
    "def show_result(moved_img,fixed_img,Ex_name):\n",
    "    stack = np.expand_dims(np.stack([moved_img,fixed_img],-1),0)\n",
    "    start = time.time()\n",
    "    u = model(stack)\n",
    "    print(time.time()-start)\n",
    "    warped_img = tf.squeeze(dense_image_warp(np.expand_dims(np.expand_dims(moved_img,-1),0),u))\n",
    "    nof = count_fold_fn(u)\n",
    "    #nof_inv = count_fold_fn(-u)\n",
    "    rel_ssd = np.sum((warped_img-fixed_img)**2)/np.sum((moved_img-fixed_img)**2)\n",
    "    print('folding  u=',nof)\n",
    "    #print('folding -u=',nof_inv)\n",
    "    print('relative ssd =',rel_ssd)\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.subplot(231)\n",
    "    plt.title(Ex_name+': Moved image')\n",
    "    plt.imshow(moved_img,cmap='gray')\n",
    "    plt.subplot(232)\n",
    "    plt.title(Ex_name+': Fixed image')\n",
    "    plt.imshow(fixed_img,cmap='gray')\n",
    "    plt.subplot(233)\n",
    "    plt.title(Ex_name+': Warped image')\n",
    "    plt.imshow(warped_img,cmap='gray')\n",
    "    plt.subplot(234)\n",
    "    plt.title(Ex_name+': Different image')\n",
    "    plt.imshow(abs(warped_img-fixed_img),cmap='gray')\n",
    "    plt.subplot(235)\n",
    "    plt.title(Ex_name+': Grid warped u')\n",
    "    grid_warp(u[0])\n",
    "    plt.subplot(236)\n",
    "    plt.title(Ex_name+': Vector field u')\n",
    "    vector_field(-u[0])\n",
    "    plt.show()\n",
    "    return rel_ssd,nof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a95012",
   "metadata": {},
   "source": [
    "# Train/Test spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07b53bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "data_set = np.load('550m2f_Large_Deform.npy')\n",
    "train_set = data_set[:500,:,:,:]\n",
    "test_set = data_set[500:550,:,:,:]\n",
    "train_size = train_set.shape[0]\n",
    "test_size = test_set.shape[0]\n",
    "img_shape = train_set.shape[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1846d7d3",
   "metadata": {},
   "source": [
    "# Loss definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96e6e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_MSE(img1,img2):\n",
    "    mse = tf.reduce_mean(tf.square(img1-img2),axis=[1,2,3])\n",
    "    return tf.cast(tf.reduce_mean(mse),tf.float32)\n",
    "\n",
    "def D_SSD(img1,img2):\n",
    "    ssd = tf.reduce_sum(tf.square(img1-img2),axis=[1,2,3])\n",
    "    return tf.cast(tf.reduce_mean(ssd),tf.float32)\n",
    "\n",
    "def D_CC(img1,img2,n=9):\n",
    "    gs_img1 = tfa.image.gaussian_filter2d(img1,filter_shape=(n,n))\n",
    "    gs_img2 = tfa.image.gaussian_filter2d(img2,filter_shape=(n,n))\n",
    "    dif_img1 = img1-gs_img1\n",
    "    dif_img2 = img2-gs_img2\n",
    "    cc = tf.reduce_sum(dif_img1*dif_img2,axis=[1,2,3])**2\n",
    "    cc = cc/(tf.reduce_sum(dif_img1**2,axis=[1,2,3])*tf.reduce_sum(dif_img2**2,axis=[1,2,3]))\n",
    "    return tf.cast(tf.reduce_mean(cc),tf.float32)\n",
    "\n",
    "def D_NGF(img1,img2,e=1):\n",
    "    img1_x,img1_y = img_grad(img1)\n",
    "    img2_x,img2_y = img_grad(img2)\n",
    "    inner = lambda a,b: a*b+e**2\n",
    "    norm_s = lambda a,b: inner(a,a)+inner(b,b)\n",
    "    ngf = 1 - ((inner(img1_x,img2_x)+inner(img1_y,img2_y))**2)/(norm_s(img1_x,img1_y)*norm_s(img2_x,img2_y))\n",
    "    return tf.reduce_mean(tf.reduce_sum(ngf,axis=[1,2,3]))\n",
    "\n",
    "def D_CLM(img1,img2):\n",
    "    img1 = tf.convert_to_tensor(img1)\n",
    "    img2 = tf.convert_to_tensor(img2)\n",
    "    b,h,w,c = img1.get_shape()\n",
    "    tile_fn = lambda b_c: tf.transpose(tf.reshape(tf.tile(b_c,(h,w)),[h,b,w,c]),[1,0,2,3])\n",
    "    img1_mean = tile_fn(tf.math.reduce_mean(img1,axis=[1,2]))\n",
    "    img2_mean = tile_fn(tf.math.reduce_mean(img2,axis=[1,2]))\n",
    "    img1_std = tile_fn(tf.math.reduce_std(img1,axis=[1,2]))\n",
    "    img2_std = tile_fn(tf.math.reduce_std(img2,axis=[1,2]))\n",
    "    img12_std = tile_fn(tf.math.reduce_std(img1+img2,axis=[1,2]))\n",
    "    clm = tf.reduce_sum(((img1-img1_mean)/img1_std-(img2-img2_mean)/img2_std)**2,axis=(1,2,3))+(img1_std+img2_std-img12_std)**2\n",
    "    return tf.cast(tf.reduce_mean(clm),tf.float32)\n",
    "    \n",
    "def R_diff(u):\n",
    "    u_x,u_y = img_grad(u)\n",
    "    difsn = tf.reduce_sum(u_x**2+u_y**2,axis=[1,2,3])\n",
    "    return tf.reduce_mean(difsn)\n",
    "\n",
    "def R_tv(u,e=5e-3):\n",
    "    u_x,u_y = img_grad(u)\n",
    "    tv = tf.reduce_sum((u_x**2+u_y**2+e)**0.5,axis=[1,2,3])\n",
    "    return tf.reduce_mean(tv)\n",
    "\n",
    "def R_curv(u):\n",
    "    u_x,u_y = tf.image.image_gradients(u) #forward different\n",
    "    u_xx = u_x-tf.roll(u_x,1,1)\n",
    "    u_yy = u_y-tf.roll(u_y,1,2)\n",
    "    curv = tf.reduce_sum(u_xx**2+u_yy**2,axis=[1,2,3])\n",
    "    return tf.reduce_mean(curv)\n",
    "\n",
    "def R_Han(u):\n",
    "    u_x,u_y = img_grad(u)\n",
    "    r = (u_x[:,:,:,0]-u_y[:,:,:,1])**2+(u_x[:,:,:,1]+u_y[:,:,:,0])**2\n",
    "    R1 = tf.reduce_sum(r,axis=[1,2])\n",
    "    return tf.reduce_mean(R1)\n",
    "\n",
    "def R_SA(u):\n",
    "    u = tf.convert_to_tensor(u)\n",
    "    u_x,u_y = img_grad(u)\n",
    "    u1_x1 = u_x[:,:,:,0]\n",
    "    u1_x2 = u_x[:,:,:,1]\n",
    "    u2_x1 = u_y[:,:,:,0]\n",
    "    u2_x2 = u_y[:,:,:,1]\n",
    "    s = u1_x2*u2_x1-u1_x1*u2_x2\n",
    "    a = abs(u1_x1+u2_x2)\n",
    "    Rsa = tf.reduce_sum(tf.minimum(0,s+a)**2,axis=[1,2])    \n",
    "    return tf.reduce_mean(Rsa)\n",
    "\n",
    "def R_fold(u):\n",
    "    u = tf.convert_to_tensor(u)\n",
    "    u_x,u_y = img_grad(u)\n",
    "    det_phi = (u_x[:,:,:,0]+1)*(u_y[:,:,:,1]+1)-u_x[:,:,:,1]*u_y[:,:,:,0]\n",
    "    det_phi_p = tf.maximum(det_phi,1)\n",
    "    fold = tf.reduce_sum((det_phi-1)**2/det_phi_p**2,axis=(1,2))\n",
    "    return tf.reduce_mean(fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5ae5050",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Unet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 2) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 256, 256, 32) 608         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    multiple             0           conv2d[0][0]                     \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 64) 18496       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 128)  73856       max_pooling2d[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 256)  295168      max_pooling2d[2][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 256)  590080      max_pooling2d[3][0]              \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d (UpSampling2D)    multiple             0           conv2d_4[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       multiple             0           up_sampling2d[0][0]              \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "                                                                 up_sampling2d[1][0]              \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "                                                                 up_sampling2d[2][0]              \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "                                                                 up_sampling2d[3][0]              \n",
      "                                                                 conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 128)  589952      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 64)   147520      concatenate[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 32) 36896       concatenate[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 256, 256, 64) 36928       concatenate[3][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 256, 256, 2)  130         conv2d_8[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,789,634\n",
      "Trainable params: 1,789,634\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "inputs = layers.Input(shape=train_set[0].shape)\n",
    "Conv2D = lambda l: layers.Conv2D(l,3,activation='relu',padding='same')\n",
    "Maxpool2D = layers.MaxPool2D()\n",
    "UpSampling2D = layers.UpSampling2D()\n",
    "Concatenate = layers.Concatenate()\n",
    "n = 4\n",
    "lys = 32\n",
    "\n",
    "x = inputs\n",
    "skip = []\n",
    "for i in range(n): \n",
    "    x = Conv2D(lys*2**i)(x)\n",
    "    skip.append(x)\n",
    "    x = Maxpool2D(x)\n",
    "for j in range(n):\n",
    "    x = Conv2D(lys*2**(n-j-1))(x)\n",
    "    x = UpSampling2D(x)\n",
    "    x = Concatenate([x,skip.pop()])\n",
    "    \n",
    "x = Conv2D(2*lys)(x)\n",
    "x = tf.keras.layers.Conv2D(2,1,padding='same')(x)\n",
    "model = Model(inputs=inputs, outputs=x,name='Unet')\n",
    "model.summary()\n",
    "weight_save = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ef6321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.005\n",
    "beta = 0.1\n",
    "name = 'e1diff'+str(alpha)#+'Han'+str(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7882cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model,imgs):\n",
    "    u = model(imgs)\n",
    "    ref_img = imgs[:,:,:,1:]\n",
    "    warped_img = dense_image_warp(imgs[:,:,:,:1],u)\n",
    "    D = D_SSD(ref_img,warped_img)\n",
    "    R = alpha*R_diff(u)#+beta*R_Han(u)\n",
    "    #print('No.folding :',count_fold_fn(u))\n",
    "    loss = D+R\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49027d8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.165952\n",
      "memory current : 8.484608, peak : 10.995456\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "epoch 1, loss_train 2067.61719, loss_test 1543.13098\n",
      "memory current : 37.147136, peak : 1851.527168\n",
      "epoch 2, loss_train 886.89502, loss_test 686.08777\n",
      "memory current : 37.029888, peak : 1851.527168\n",
      "epoch 3, loss_train 585.01196, loss_test 456.00934\n",
      "epoch 4, loss_train 493.61658, loss_test 414.10049\n",
      "epoch 5, loss_train 451.00208, loss_test 406.67114\n",
      "epoch 6, loss_train 421.94104, loss_test 372.72849\n",
      "epoch 7, loss_train 391.0777, loss_test 357.14969\n",
      "epoch 8, loss_train 368.87634, loss_test 340.33432\n",
      "epoch 9, loss_train 352.33499, loss_test 330.14191\n",
      "epoch 10, loss_train 339.71347, loss_test 318.57043\n",
      "epoch 11, loss_train 326.71683, loss_test 312.88306\n",
      "epoch 12, loss_train 314.11905, loss_test 304.41006\n",
      "epoch 13, loss_train 302.3486, loss_test 293.10559\n",
      "epoch 14, loss_train 291.22128, loss_test 284.08505\n",
      "epoch 15, loss_train 282.17853, loss_test 271.89069\n",
      "epoch 16, loss_train 273.66943, loss_test 265.14227\n",
      "epoch 17, loss_train 266.53247, loss_test 257.94922\n",
      "epoch 18, loss_train 261.45032, loss_test 254.53462\n",
      "epoch 19, loss_train 251.98439, loss_test 251.92175\n",
      "epoch 20, loss_train 245.99269, loss_test 245.04659\n",
      "epoch 21, loss_train 241.58447, loss_test 241.57736\n",
      "epoch 22, loss_train 237.10114, loss_test 234.86014\n",
      "epoch 23, loss_train 231.84784, loss_test 236.5416\n",
      "epoch 24, loss_train 230.13824, loss_test 233.05533\n",
      "epoch 25, loss_train 226.22481, loss_test 232.45911\n",
      "epoch 26, loss_train 221.57497, loss_test 232.56018\n",
      "epoch 27, loss_train 218.0856, loss_test 226.07735\n",
      "epoch 28, loss_train 215.53632, loss_test 224.10477\n",
      "epoch 29, loss_train 212.28374, loss_test 217.23726\n",
      "epoch 30, loss_train 209.5645, loss_test 218.86685\n",
      "epoch 31, loss_train 210.12578, loss_test 218.10818\n",
      "epoch 32, loss_train 208.55798, loss_test 212.44658\n",
      "epoch 33, loss_train 210.87442, loss_test 214.91483\n",
      "epoch 34, loss_train 203.90128, loss_test 213.81796\n",
      "epoch 35, loss_train 195.91713, loss_test 202.85789\n",
      "epoch 36, loss_train 190.38295, loss_test 199.84189\n",
      "epoch 37, loss_train 186.8801, loss_test 194.78125\n",
      "epoch 38, loss_train 182.07182, loss_test 188.87479\n",
      "epoch 39, loss_train 179.37186, loss_test 188.66562\n",
      "epoch 40, loss_train 177.84767, loss_test 190.91765\n",
      "epoch 41, loss_train 178.20978, loss_test 184.96796\n",
      "epoch 42, loss_train 177.46719, loss_test 200.80789\n",
      "epoch 43, loss_train 176.52461, loss_test 186.16959\n",
      "epoch 44, loss_train 175.446, loss_test 180.25568\n",
      "epoch 45, loss_train 174.3748, loss_test 186.62444\n",
      "epoch 46, loss_train 175.86411, loss_test 189.29056\n",
      "epoch 47, loss_train 170.90752, loss_test 186.69531\n",
      "epoch 48, loss_train 169.31778, loss_test 184.48967\n",
      "epoch 49, loss_train 171.32179, loss_test 189.23822\n",
      "epoch 50, loss_train 173.30782, loss_test 194.78522\n",
      "epoch 51, loss_train 170.82924, loss_test 192.49294\n",
      "epoch 52, loss_train 170.82108, loss_test 180.48161\n",
      "epoch 53, loss_train 171.15184, loss_test 190.43192\n",
      "epoch 54, loss_train 173.08432, loss_test 186.32384\n",
      "epoch 55, loss_train 178.1953, loss_test 176.75682\n",
      "epoch 56, loss_train 166.5997, loss_test 172.23453\n",
      "epoch 57, loss_train 162.53992, loss_test 168.34561\n",
      "epoch 58, loss_train 157.18427, loss_test 159.68231\n",
      "epoch 59, loss_train 154.08505, loss_test 160.95688\n",
      "epoch 60, loss_train 153.39229, loss_test 156.8107\n",
      "epoch 61, loss_train 151.01468, loss_test 155.81523\n",
      "epoch 62, loss_train 148.27415, loss_test 163.67503\n",
      "epoch 63, loss_train 149.36313, loss_test 154.4213\n",
      "epoch 64, loss_train 148.54996, loss_test 151.78807\n",
      "epoch 65, loss_train 144.59476, loss_test 149.4837\n",
      "epoch 66, loss_train 143.3998, loss_test 148.09712\n",
      "epoch 67, loss_train 140.37202, loss_test 146.37497\n",
      "epoch 68, loss_train 143.09496, loss_test 149.34987\n",
      "epoch 69, loss_train 141.84151, loss_test 149.53993\n",
      "epoch 70, loss_train 148.16246, loss_test 172.58365\n",
      "epoch 71, loss_train 153.77695, loss_test 156.09879\n",
      "epoch 72, loss_train 151.06876, loss_test 164.88571\n",
      "epoch 73, loss_train 154.48692, loss_test 180.401\n",
      "epoch 74, loss_train 149.50061, loss_test 150.93808\n",
      "epoch 75, loss_train 139.4698, loss_test 149.05606\n",
      "epoch 76, loss_train 141.08682, loss_test 163.32744\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.experimental.get_memory_info('GPU:0')['current']/1e6)\n",
    "model.set_weights(weight_save)\n",
    "epochs = 100\n",
    "batch_size = 10\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "loss_log = []\n",
    "metric_log = []\n",
    "start_train = time.time()\n",
    "for epoch in range(epochs):\n",
    "    if epoch<3:\n",
    "        mem = tf.config.experimental.get_memory_info('GPU:0')\n",
    "        cr,pk = mem['current'],mem['peak']\n",
    "        print('memory current : {}, peak : {}'.format(cr/1e6,pk/1e6)) \n",
    "    loss_epoch = []\n",
    "    test_epoch = []\n",
    "    #train\n",
    "    for i in range(0,train_size,batch_size):\n",
    "        train_batch = train_set[i:i+batch_size]\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_batch = loss_fn(model,train_batch)\n",
    "            grads = tape.gradient(loss_batch, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        loss_epoch.append(loss_batch.numpy())\n",
    "    #test\n",
    "    for j in range(0,test_size,batch_size):\n",
    "        loss_test = loss_fn(model,test_set[j:j+batch_size])\n",
    "        test_epoch.append(loss_batch.numpy())\n",
    "    loss_log = np.append(loss_log,[np.mean(loss_epoch),np.mean(test_epoch)])\n",
    "     \n",
    "    dgt = 5 #display 'dgt' digits\n",
    "    print('epoch {}, loss_train {}, loss_test {}'.format(epoch+1,np.round(loss_log[-2],dgt),np.round(loss_log[-1],dgt)))\n",
    "print(time.time()-start_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932bb5d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.load_weights('./weight models/'+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f89ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_log = loss_log.reshape(loss_log.size//2,2)\n",
    "plt.plot(loss_log[:,0],label='train')\n",
    "plt.plot(loss_log[:,1],label='test')\n",
    "plt.title(name)\n",
    "plt.ylim(0,max(loss_log[0,:]))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399467e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ind_list = choice(test_set.shape[0],3,False)\n",
    "for ind in ind_list:\n",
    "    test = np.expand_dims(test_set[ind],0)\n",
    "    moved_img = test[0,:,:,0]\n",
    "    fixed_img = test[0,:,:,1]\n",
    "    show_result(moved_img,fixed_img,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab782647",
   "metadata": {},
   "outputs": [],
   "source": [
    "circle = cv2.resize(cv2.imread('./Images/circle.png',0),img_shape)/255\n",
    "ellipse = cv2.resize(cv2.imread('./Images/ellipse.png',0),img_shape)/255\n",
    "square = cv2.resize(cv2.imread('./Images/square.png',0),img_shape)/255\n",
    "c_shape = cv2.resize(cv2.imread('./Images/c_shape.png',0),img_shape)/255\n",
    "o_ = cv2.resize(cv2.imread('./Images/o_.png',0),img_shape)/255\n",
    "R1 = cv2.resize(cv2.imread('./Images/R1.png',0),img_shape)/255\n",
    "T1 = cv2.resize(cv2.imread('./Images/T1.png',0),img_shape)/255\n",
    "t2_R = cv2.resize(cv2.imread('./Images/t2_R.png',0),img_shape)/255\n",
    "t2_T = cv2.resize(cv2.imread('./Images/t2_T.png',0),img_shape)/255\n",
    "R2 = cv2.resize(cv2.imread('./Images/R2.png',0),img_shape)/255\n",
    "T2 = cv2.resize(cv2.imread('./Images/T2.png',0),img_shape)/255\n",
    "A_slope = cv2.resize(cv2.imread('./Images/A_slope.png',0),img_shape)/255\n",
    "A = cv2.resize(cv2.imread('./Images/A.png',0),img_shape)/255\n",
    "R = cv2.resize(cv2.imread('./Images/R.png',0),img_shape)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2773812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_list = [\n",
    "    [ellipse,circle],\n",
    "    [circle,ellipse],\n",
    "    [circle,square],\n",
    "    [square,circle],\n",
    "    [t2_R,t2_T],\n",
    "    [t2_T,t2_R],\n",
    "    [o_,c_shape],\n",
    "    [c_shape,o_],\n",
    "    [A,R],\n",
    "    [R,A],\n",
    "    [R1,T1],\n",
    "    [T1,R1],\n",
    "    [R2,T2],\n",
    "    [T2,R2]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea59398a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for i in display_list:\n",
    "    result_list.append(show_result(i[0],i[1],name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ac2180",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "result = np.array(result_list).transpose()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca94f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_ssd_list = []\n",
    "nof_list = []\n",
    "for j in range(0,test_size,batch_size):\n",
    "    test_batch = test_set[j:j+batch_size,:,:,:]\n",
    "    u = model(test_batch)\n",
    "    moved_imgs = test_batch[:,:,:,:1]\n",
    "    warped_imgs = dense_image_warp(moved_imgs,u)\n",
    "    fixed_imgs = test_batch[:,:,:,1:]\n",
    "    rel_ssd_list.append(tf.reduce_sum((warped_imgs-fixed_imgs)**2,axis=[1,2])/tf.reduce_sum((moved_imgs-fixed_imgs)**2,axis=[1,2]))\n",
    "    nof_list.append(count_fold_fn(u))\n",
    "print('rel_ssd :',np.mean(rel_ssd_list))\n",
    "print('No.folding :',np.mean(nof_list))\n",
    "print('rel_ssd STD:',np.std(rel_ssd_list))\n",
    "print('No.folding STD:',np.std(nof_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f490a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./loss log/'+name+'.npy',loss_log)\n",
    "model.save_weights('./weight models/'+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428388b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow2-GPU",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
